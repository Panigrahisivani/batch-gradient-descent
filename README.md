# ðŸŒˆ Batch Gradient Descent From Scratch

A complete, from-scratch implementation of **Batch Gradient Descent** for Linear Regression using Python and NumPy.  
This project includes **single-variate** and **multi-variate** models, performance comparison with scikit-learn, and multiple high-quality visualizations.

---

## ðŸ“Š Visual Results

Below are some of the key plots generated from the gradient descent implementation:

![Contour Plot of Cost Function](plots/Contour_plot_of_Cost_Function_2D.png)
![3D Cost Surface Bowl](plots/Cost_Function_Surface_Bowl_3D.png)
![Loss Curve (Multi-Variate GD)](plots/Gradient_Descent_Loss_Curve_Multi_Variate.png)
![Loss Curve (Single-Variate GD)](plots/Gradient_Descent_Loss_Curve_Single_Variate.png)
![Residual Distribution (GD vs Sklearn)](plots/Residual_Distribution_GD_vs_Sklearn.png)

---

## ðŸ“˜ Overview

Batch Gradient Descent updates the model parameters by evaluating the gradient of the cost function using **all samples** in the dataset for every iteration.  
This method is stable, deterministic, and widely used for linear regression problems.

---

## ðŸ”§ Features Implemented

- Single-variate Batch Gradient Descent  
- Multi-variate Batch Gradient Descent  
- Loss curve visualization  
- Actual vs predicted comparison  
- Residual distribution analysis  
- 2D contour & 3D cost surface plots  
- Comparison with scikit-learn's LinearRegression  

---

## ðŸŽ¯ Summary

This project demonstrates how Batch Gradient Descent works at a fundamental level, including:

- Cost surface interpretation  
- Parameter convergence behavior  
- Residual error behavior  
- Comparison of custom GD vs scikit-learn  

It serves as a strong foundational reference for understanding optimization in machine learning.

---

